# -*- coding: utf-8 -*-
"""HeartDisease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jD446DVHNlylkliIIF_AK2kNEoU8qgyN

# Heart Disease Prediction

## Submission 1 - Machine Learning Terapan
Nama    : Andre Saputra Ginting

dataset : https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction

# Latar Belakang
Penyakit jantung merupakan penyebab utama kematian di dunia dan sering tidak terdeteksi hingga gejala serius muncul atau terjadi serangan jantung mendadak tanpa peringatan. Kondisi ini disebut sebagai "penyakit tersembunyi" karena gejala awal seperti kelelahan, nyeri dada ringan, atau sesak napas sering disalahartikan sebagai masalah ringan, sehingga banyak penderita tidak memperoleh diagnosis dan pengobatan tepat waktu. Kurangnya kesadaran ini meningkatkan risiko komplikasi serius yang sebenarnya dapat dicegah dengan deteksi dini dan penanganan yang tepat (Columbia Asia, 2024).

Penyakit jantung dikenal sebagai salah satu penyebab utama kematian di dunia, termasuk di Indonesia. Berdasarkan data dari World Health Organization (WHO) tahun 2021, tercatat sekitar 17,8 juta kematian setiap tahunnya disebabkan oleh penyakit jantung, atau setara dengan satu dari tiga kematian secara global. Jumlah kasus penyakit jantung terbaru mencapai 21,2 juta, dengan prevalensi lebih tinggi pada laki-laki dibandingkan perempuan (Kementerian Kesehatan RI, 2021).

Penggunaan teknologi seperti machine learning dan analisis data kesehatan berperan penting dalam deteksi dini penyakit jantung. Dengan menganalisis data klinis dan gaya hidup, algoritma dapat mengenali pola dan faktor risiko secara akurat. Model prediktif ini membantu tenaga medis membuat keputusan lebih cepat dan tepat, serta memungkinkan pemantauan kesehatan secara berkala untuk mencegah kondisi memburuk. Pendekatan ini mendukung sistem kesehatan yang lebih preventif dan berpotensi menurunkan angka kematian akibat penyakit jantung.

#Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('display.max_columns', None)

!pip install opendatasets
import opendatasets as od

od.download('https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction', force=True)

"""# Load Dataset"""

df = pd.read_csv('/content/heart-failure-prediction/heart.csv')
df.head()

"""# Data Understanding"""

# List untuk menyimpan ringkasan per kolom
list_of_descs = []

for col in df.columns:
    col_type = df[col].dtype
    total_row = df.shape[0]
    null_count = df[col].isna().sum()
    null_pct = round((null_count / total_row) * 100, 2)
    unique_count = df[col].nunique()
    sample_unique = df[col].unique()

    list_of_descs.append([
        col,
        col_type,
        total_row,
        null_count,
        null_pct,
        unique_count,
        sample_unique
    ])

# Buat dataframe ringkasan
df_desc = pd.DataFrame(
    list_of_descs,
    columns=[
        'Column',
        'Type',
        'Total Rows',
        'Null Count',
        'Null %',
        'Unique Values',
        'Sample Unique Values'
    ]
)

# Tampilkan ringkasan
df_desc

"""Dari hasil analisis awal deskripsi data, terlihat bahwa data siap untuk tahap selanjutnya.

# Exploratory Data Analysis

## Univariate Analysis
"""

cat_columns = df.select_dtypes(include='object').columns
num_columns = df.select_dtypes(include=np.number).columns.drop('HeartDisease')

print(f'Categorical Column: {cat_columns}')
print(f'Numerical Column: {num_columns}')

# Visualisasi Distribusi Target
plt.figure(figsize=(6, 6))

# Buat Series dengan label yang diubah
target_counts = df['HeartDisease'].map({0: 'No', 1: 'Yes'}).value_counts()

target_counts.plot.pie(autopct='%1.1f%%', startangle=90, colors=['skyblue', 'salmon'])
plt.title('Distribusi Data Target (HeartDisease)')
plt.ylabel('')
plt.show()

"""Distribusi data pada label `HeartDisease` adalah cukup imbalance (55.3% vs 44.7%). Hal ini harus diperhatikan saat pelatihan model, agar tidak menghasilkan prediksi yang berat sebelah. Metrik evaluasi yang tepat dan teknik penyeimbangan data seperti oversampling bisa membantu mengatasi masalah ini."""

plt.figure(figsize=(22,12))
plt.style.use('ggplot')

# Loop untuk membuat subplot
for i, col in enumerate(cat_columns):
    plt.subplot(4, 3, i + 1)
    sns.countplot(data=df, x=col, color='blue')
    plt.title(f'{col}', fontsize=12)
    plt.xlabel('')
    plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""Distribusi data kategorikal menunjukkan ketimpangan pada beberapa fitur, seperti *ChestPainType* dan *ST\_Slope*. Ketimpangan ini dapat memengaruhi kinerja model, sehingga perlu penanganan khusus. Salah satu cara efektif adalah menggunakan **One-Hot Encoding**, yang mengubah setiap kategori menjadi kolom biner tanpa memberi makna urutan. Pendekatan ini membantu model memahami setiap kategori secara adil dan mencegah bias terhadap kategori mayoritas.

"""

plt.figure(figsize=(22,24))
plt.style.use('ggplot')

for i, col in enumerate(num_columns):
    plt.subplot(3, 3, i + 1)
    sns.histplot(data=df, x=col, color='blue')
    plt.title(f'{col}', fontsize=12)
    plt.xlabel('')
    plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""Distribusi data kategorikal menunjukkan ketimpangan pada beberapa fitur, seperti `ChestPainType` dan `ST_Slope`. Ketimpangan ini dapat memengaruhi kinerja model, sehingga perlu penanganan khusus. Salah satu cara efektif adalah menggunakan **One-Hot Encoding**, yang mengubah setiap kategori menjadi kolom biner tanpa memberi makna urutan. Pendekatan ini membantu model memahami setiap kategori secara adil dan mencegah bias terhadap kategori mayoritas.

"""

plt.figure(figsize=(22,24))
plt.style.use('ggplot')

for i, col in enumerate(num_columns):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(data=df, x=col, color='blue')
    plt.title(f'{col}', fontsize=12)
    plt.xlabel('')
    plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""Boxplot menunjukkan bahwa sebagian besar fitur memiliki **outlier**, terutama pada `RestingBP`, `Cholesterol`, dan `Oldpeak`, sedangkan `Age` dan `MaxHR` memiliki sebaran yang relatif normal. Fitur `FastingBS` didominasi oleh nilai 0 dan minim variasi, sehingga dianggap tidak informatif dan akhirnya dihapus karena berpotensi tidak berkontribusi signifikan terhadap performa model."""

plt.figure(figsize=(12, 12))
sns.heatmap(df[num_columns].corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.tight_layout()
plt.title("Correlation Matrix Numerical Columns")
plt.show()

"""Berdasarkan korelasi antar fitur numerik, tidak ditemukan fitur yang redundant dalam dataset ini. Nilai korelasi antar fitur mayoritas rendah, di bawah 0.3, sehingga setiap fitur memberikan informasi yang unik dan layak dipertahankan untuk analisis atau pemodelan."""

df.drop(['FastingBS'], axis=1, inplace=True)

"""##Multivariate Analysis"""

plt.figure(figsize=(22, 12))
plt.style.use('ggplot')

for i, col in enumerate(cat_columns):
    plt.subplot(4, 3, i + 1)
    ax = sns.countplot(data=df, x=col, hue='HeartDisease', palette='viridis')
    plt.title(f'{col} vs HeartDisease', fontsize=12)
    plt.xlabel('')
    plt.ylabel('Count')
    plt.legend(title='HeartDisease', labels=['No', 'Yes'])

    # Dapatkan jumlah kategori unik pada sumbu x
    n_categories = len(df[col].unique())
    # n_hue = 2 (HeartDisease: No, Yes)

    # Karena countplot dengan hue, jumlah bar = n_categories * n_hue
    n_hue = len(df['HeartDisease'].unique())

    # Bar patches diurutkan per kategori x dan per hue
    patches = ax.patches

    # Loop untuk tiap kategori
    for j in range(n_categories):
        # Index bar untuk kategori ke-j: dari j*n_hue sampai (j+1)*n_hue - 1
        start_idx = j * n_hue
        end_idx = start_idx + n_hue

        # Hitung total tinggi bar untuk kategori j
        total = sum(p.get_height() for p in patches[start_idx:end_idx])

        # Tambahkan label persentase di tiap bar kategori j
        for k in range(start_idx, end_idx):
            height = patches[k].get_height()
            if total > 0:
                percent = height / total * 100
                patches[k].set_height(height)  # optional, biar pasti
                ax.text(
                    patches[k].get_x() + patches[k].get_width() / 2,
                    height + 5,
                    f'{percent:.1f}%',
                    ha='center',
                    fontsize=9
                )

plt.tight_layout()
plt.show()

"""Berikut adalah penjelasan pada **indikasi** pengaruh variabel kategorikal terhadap distribusi **HeartDisease** :

### 1. **Sex vs HeartDisease**

* Pria (M) cenderung memiliki jumlah kasus penyakit jantung (HeartDisease = Yes) sebesar 90.2% yang jauh lebih banyak dibandingkan wanita (F) yang sebesar 9.8%.
* Wanita memiliki jumlah kasus negatif sebesar 34.9% (tidak ada penyakit jantung) yang lebih tinggi dibandingkan kasus positif yang sebesar 9.8%.

Indikasi: **Jenis kelamin pria mungkin terkait dengan risiko lebih tinggi terkena penyakit jantung**.

### 2. **ChestPainType vs HeartDisease**

* Tipe nyeri dada **ASY (Asymptomatic)** tampak terkait dengan jumlah kasus penyakit jantung yang cukup tinggi.
* Tipe ATA (Atypical angina), NAP (Non-anginal pain), dan TA (Typical angina) lebih sering muncul pada pasien tanpa penyakit jantung.

Indikasi: **Nyeri dada tipe Asymptomatic berpotensi menjadi indikator kuat penyakit jantung**.

### 3. **RestingECG vs HeartDisease**

* Pada kategori **Normal**, distribusi pasien dengan dan tanpa penyakit jantung tampak seimbang.
* Kategori **ST** dan **LVH** lebih banyak ditemukan pada pasien dengan penyakit jantung.

Indikasi: **Abnormalitas pada Resting ECG (ST, LVH) bisa mengindikasikan adanya penyakit jantung**.

### 4. **ExerciseAngina vs HeartDisease**

* Pasien dengan **ExerciseAngina = Y (ya)** cenderung lebih banyak yang positif penyakit jantung.
* Sebaliknya, pasien tanpa exercise angina (N) mayoritas tidak memiliki penyakit jantung.

Indikasi: **Exercise angina mungkin menjadi tanda yang cukup kuat adanya penyakit jantung**.

### 5. **ST\_Slope vs HeartDisease**

* Kategori **Flat** pada ST slope tampak terkait dengan jumlah kasus penyakit jantung yang tinggi.
* Kategori **Up** lebih sering ditemukan pada pasien tanpa penyakit jantung.
* Kategori **Down** juga lebih banyak pada pasien dengan penyakit jantung, meskipun jumlahnya lebih kecil dari Flat.

Indikasi: **ST slope Flat dan Down bisa menjadi indikator risiko penyakit jantung yang lebih tinggi dibandingkan slope Up**.
"""

num_columns = df.select_dtypes(include=np.number).columns.drop('HeartDisease')

plt.figure(figsize=(22, 24))
plt.style.use('ggplot')

for i, col in enumerate(num_columns):
    plt.subplot(3, 3, i + 1)
    sns.histplot(data=df, x=col, hue='HeartDisease', multiple='stack', palette='viridis', kde=True)
    plt.title(f'Distribution of {col} by HeartDisease', fontsize=12)
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.legend(title='HeartDisease', labels=['No', 'Yes'])

    # Mean untuk masing-masing grup dan keseluruhan
    mean_all = df[col].mean()
    mean_0 = df[df['HeartDisease'] == 0][col].mean()
    mean_1 = df[df['HeartDisease'] == 1][col].mean()

    # Garis vertikal
    plt.axvline(mean_all, color='black', linestyle='--', linewidth=2, label='Mean (All)')
    plt.axvline(mean_0, color='blue', linestyle='-', linewidth=1.5, label='Mean (No)')
    plt.axvline(mean_1, color='red', linestyle='-', linewidth=1.5, label='Mean (Yes)')

    # Teks rata-rata
    ymax = plt.ylim()[1]
    xpos = plt.xlim()[1] * 0.6
    plt.text(xpos, ymax * 0.85, f'Mean (All): {mean_all:.2f}', fontsize=10, color='black')
    plt.text(xpos, ymax * 0.75, f'Mean (No): {mean_0:.2f}', fontsize=10, color='blue')
    plt.text(xpos, ymax * 0.65, f'Mean (Yes): {mean_1:.2f}', fontsize=10, color='red')

plt.tight_layout()
plt.show()

"""**1. Age vs HeartDisease**
- Distribusi: Terlihat pergeseran ke kanan pada grup penderita HeartDisease = Yes, artinya penderita cenderung lebih tua dibandingkan yang tidak.
- Rata-rata:
  - All: 53.51
  - No: 50.55
  - Yes: 54.90

Pengaruh: Umur yang lebih tua meningkatkan kemungkinan terkena penyakit jantung, karena risiko penyakit jantung memang meningkat seiring bertambahnya usia.

**2. RestingBP vs HeartDisease**
- Distribusi: Kedua kelompok (HeartDisease = No dan Yes) memiliki distribusi yang mirip, namun terdapat sedikit pergeseran ke kanan pada kelompok HeartDisease = Yes, yang menunjukkan tekanan darah lebih tinggi.
- Rata-rata:
  - All: 132.40
  - No: 130.18
  - Yes: 134.19

Pengaruh: enderita penyakit jantung cenderung memiliki tekanan darah istirahat yang sedikit lebih tinggi. Meskipun perbedaannya tidak terlalu besar, tekanan darah yang meningkat dapat menjadi faktor risiko tambahan terhadap penyakit jantung, terutama jika dikombinasikan dengan faktor risiko lainnya.

**3. Cholesterol vs HeartDisease**
- Distribusi: Terlihat bahwa kelompok HeartDisease = No memiliki distribusi kolesterol yang lebih tinggi dibandingkan dengan kelompok HeartDisease = Yes, yang cukup mengejutkan karena bertentangan dengan asumsi umum.
- Rata-rata:
  - All: 198.80
  - No: 227.12
  - Yes: 175.94

Pengaruh: Secara mengejutkan, kolesterol rata-rata pada penderita penyakit jantung justru lebih rendah. Hal ini bisa disebabkan oleh berbagai faktor, seperti efek pengobatan yang menurunkan kolesterol, gaya hidup setelah diagnosis, atau distribusi data yang tidak merata. Oleh karena itu, kolesterol dalam dataset ini mungkin kurang representatif sebagai indikator tunggal untuk mendeteksi penyakit jantung.

**4. MaxHR vs HeartDisease**
- Distribusi: Distribusi menunjukkan bahwa kelompok HeartDisease = No memiliki detak jantung maksimum yang lebih tinggi, sedangkan kelompok HeartDisease = Yes cenderung memiliki MaxHR yang lebih rendah.
- Rata-rata:
  - All: 136.81
  - No: 148.15
  - Yes: 127.66

Pengaruh: Penderita penyakit jantung cenderung tidak mampu mencapai detak jantung maksimum yang tinggi saat beraktivitas fisik, yang bisa menjadi indikasi keterbatasan fungsi jantung. MaxHR yang lebih rendah merupakan sinyal penting adanya potensi gangguan jantung, sehingga fitur ini cukup signifikan dalam membedakan antara penderita dan non-penderita.

**5. Oldpeak vs HeartDisease**
- Distribusi: Sangat berbeda antar grup. Grup HeartDisease = Yes punya distribusi lebih menyebar ke kanan (nilai oldpeak lebih tinggi).
- Rata-rata:
  - All: 0.89
  - No: 0.41
  - Yes: 1.27

Pengaruh: Oldpeak tinggi mengindikasikan abnormalitas EKG akibat iskemia (kurangnya aliran darah ke jantung), sehingga sangat erat kaitannya dengan penyakit jantung. Ini adalah salah satu fitur yang paling membedakan kedua grup.

#Data Preprocessing
"""

df_prep = df.copy()
df_prep

"""## Handle Missing Values"""

df_prep.isna().sum()

"""Pada dataset terlihat tidak terdapat missing value.

## Handle Duplicated Values
"""

print(f'Jumlah data duplikat:', df_prep.duplicated().sum())

"""Pada dataset terlihat tidak terdapat data duplikat.

## Handle Outlier Values
"""

col_selected = ['RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']

def remove_outlier(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

for col in col_selected:
    df_prep = remove_outlier(df_prep, col)

"""Untuk menangani outlier, dilakukan penghapusan outlier pada kolom `RestingBP`, `Cholesterol`, `MaxHR` dan `Oldpeak` menggunakan metode IQR (Interquartile Range). Metode ini diterapkan untuk menghapus data yang berada di luar rentang batas bawah dan batas atas yang telah ditetapkan, sehingga menghasilkan data yang lebih rapi dan menggambarkan kondisi sebenarnya dengan lebih baik.

## Encoding Values
"""

df_prep.head()

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# Label Encoding untuk kolom 'Sex' dan 'ExerciseAngina'
le_col = ['Sex',  'ExerciseAngina']

le = LabelEncoder()

for col in le_col:
    df_prep[col] = le.fit_transform(df_prep[col])

# Label Encoding untuk kolom 'ChestPainType', 'RestingECG' dan 'ST_Slope'
ohe_col = ['ChestPainType', 'RestingECG', 'ST_Slope']
df_prep = pd.get_dummies(df_prep, columns=ohe_col, dtype='int')

"""Di tahap ini, data kategori berupa teks atau label diubah menjadi format angka supaya bisa diproses oleh algoritma machine learning. Proses encoding pada fitur kategorikal dilakukan dalam dua tahap:

1. **Label Encoding**: mengubah nilai kategori menjadi bilangan bulat (seperti `0` dan `1`) pada variabel yang hanya memiliki dua kelas, berikut adalah kolomnya:
   - `Sex`
   - `ExerciseAngina`

2. One Hot Encoding: mengonversi setiap kategori menjadi kolom-kolom biner yang terpisah, digunakan untuk data kategori yang tidak memiliki urutan, berikut adalah kolomnya:
   - `ChestPainType`
   - `RestingECG`
   - `ST_Slope`
"""

df_prep.head()

df_prep.shape

from sklearn.model_selection import train_test_split

X = df_prep.drop('HeartDisease', axis=1)
y = df_prep['HeartDisease']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Data dibagi dengan rasio 80:20, di mana 80% dari data digunakan sebagai data pelatihan (training) untuk membangun dan mengoptimalkan model, sedangkan 20% sisanya dialokasikan sebagai data pengujian (testing). Pembagian ini bertujuan untuk memastikan proses evaluasi model berjalan secara objektif dan hasil pengujian mencerminkan kemampuan model dalam menghadapi data baru yang belum pernah dilihat sebelumnya. Dengan demikian, performa model dapat diukur secara akurat dan generalisasi model terhadap data nyata dapat dipantau."""

print(X_train.shape)
print(X_test.shape)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Dilakukan proses scaling menggunakan MinMaxScaler untuk menormalkan rentang nilai pada setiap fitur. Dengan metode ini, semua fitur diubah ke dalam skala yang seragam, biasanya antara 0 hingga 1, sehingga tidak ada fitur yang memiliki pengaruh berlebihan akibat perbedaan skala nilai. Hal ini membantu algoritma machine learning bekerja lebih efektif dan menghasilkan model yang lebih stabil."""

print(f'Jumlah data HeartDisease bernilai yes sebelum SMOTE: {y_train.value_counts()[1]}')
print(f'Jumlah data HeartDisease bernilai no sebelum SMOTE: {y_train.value_counts()[0]}')

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_smt, y_train_smt = smote.fit_resample(X_train, y_train)

print(f'Jumlah data HeartDisease bernilai yes sesudah SMOTE: {y_train_smt.value_counts()[1]}')
print(f'Jumlah data HeartDisease bernilai no sesudah SMOTE: {y_train_smt.value_counts()[0]}')

"""Untuk mengatasi ketidakseimbangan kelas pada data latih, digunakan teknik SMOTE (Synthetic Minority Over-sampling Technique) yang menambahkan data sintetis pada kelas yang jumlahnya lebih sedikit. Selain itu, pengujian juga dilakukan pada data tanpa penerapan SMOTE guna membandingkan tingkat akurasi dan menilai sejauh mana metode ini efektif dalam meningkatkan performa model.

#Modeling

Pada tahap ini dilakukan pengujian terhadap beberapa algoritma klasifikasi, yaitu Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, AdaBoost, Support Vector Classifier (SVC), K-Nearest Neighbors, Gaussian Naive Bayes, dan XGBoost. Tujuan dari pengujian ini adalah untuk mengevaluasi dan membandingkan performa masing-masing model guna menentukan algoritma yang paling efektif dalam menangani kasus prediksi serangan jantung.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

"""Telah dibuat sebuah fungsi untuk melatih berbagai algoritma klasifikasi secara bersamaan, serta menghitung metrik performa pada data training dan data testing. Perbandingan hasil performa antara kedua data ini digunakan untuk menilai kemungkinan terjadinya overfitting atau underfitting pada masing-masing model."""

def train_model(X_train, y_train, X_test, y_test):
# Daftar model yang ingin diuji
    models = {
        "Logistic Regression": LogisticRegression(random_state=42),
        "Decision Tree": DecisionTreeClassifier(random_state=42),
        "Random Forest": RandomForestClassifier(random_state=42),
        "AdaBoost": AdaBoostClassifier(random_state=42),
        "Gradient Boosting": GradientBoostingClassifier(random_state=42),
        "Support Vector Machine": SVC(random_state=42),
        "Naive Bayes": GaussianNB(),
        "K-Nearest Neighbors": KNeighborsClassifier(),
        "XGBoost": XGBClassifier(random_state=42)
    }

    # List untuk menyimpan hasil
    results = []

    # Loop untuk training dan evaluasi tiap model
    for name, model in models.items():
        model.fit(X_train, y_train)

        # Prediksi untuk train dan test
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Simpan hasil ke list
        results.append({
            "model": name,
            "acc_train": accuracy_score(y_train, y_train_pred),
            "acc_test": accuracy_score(y_test, y_test_pred),
            "prec_train": precision_score(y_train, y_train_pred),
            "prec_test": precision_score(y_test, y_test_pred),
            "rec_train": recall_score(y_train, y_train_pred),
            "rec_test": recall_score(y_test, y_test_pred),
            "f1_train": f1_score(y_train, y_train_pred),
            "f1_test": f1_score(y_test, y_test_pred),
            "roc_train": roc_auc_score(y_train, y_train_pred),
            "roc_test": roc_auc_score(y_test, y_test_pred)
        })

    # Konversi hasil ke DataFrame
    results_df = pd.DataFrame(results, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
    return results_df

results = train_model(X_train_smt, y_train_smt, X_test, y_test)
print(f'Dengan SMOTE')
results

"""Setelah penerapan SMOTE, model **Random Forest** dan **Gradient Boosting** menunjukkan performa terbaik dengan recall di atas 93%, F1-score sekitar 92%, dan ROC AUC di atas 91%, menandakan kemampuan kuat dalam mendeteksi risiko penyakit jantung secara seimbang. Model **K-Nearest Neighbors** dan **Support Vector Machine** juga tampil kompetitif dengan recall dan F1-score tinggi. Sementara itu, **Decision Tree** mengalami overfitting dengan performa test yang lebih rendah dibanding training. Secara keseluruhan, SMOTE berhasil meningkatkan deteksi kelas minoritas, terutama pada model ensemble, sehingga meminimalkan kesalahan klasifikasi yang serius."""

results = train_model(X_train, y_train, X_test, y_test)
print(f'Tanpa SMOTE')
results

"""Penerapan SMOTE secara umum meningkatkan kemampuan model dalam mendeteksi risiko penyakit jantung, terutama terlihat pada model Gradient Boosting dan K-Nearest Neighbors yang mengalami peningkatan recall dan F1-score, menandakan lebih baiknya identifikasi kasus positif kelas minoritas. Sebaliknya, model Random Forest menunjukkan performa sedikit menurun setelah SMOTE karena sudah memiliki hasil yang sangat baik tanpa oversampling. Meskipun demikian, SMOTE berhasil menjaga keseimbangan antara metrik presisi, recall, dan ROC AUC tanpa mengorbankan akurasi secara signifikan, sehingga menjadi teknik yang efektif untuk meningkatkan sensitivitas model terhadap kelas minoritas dalam dataset yang tidak seimbang.

## Random Forest  + Hyperparameter Tuning
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report,
    confusion_matrix
)
from scipy.stats import randint
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Inisialisasi model Random Forest
rf = RandomForestClassifier(random_state=42)

# Definisikan ruang pencarian hyperparameter
param_dist = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 30),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 20),
    'max_features': ['sqrt', 'log2'],  # Hapus 'auto'
    'bootstrap': [True, False]
}

# Inisialisasi RandomizedSearchCV
random_search_rf = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=50,
    scoring='roc_auc',
    cv=5,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# Latih model dengan data training
random_search_rf.fit(X_train, y_train)

# Model terbaik
best_rf_model = random_search_rf.best_estimator_
best_rf_params = random_search_rf.best_params_

# Tampilkan hasil tuning
print("Best Hyperparameters for Random Forest:")
print(best_rf_params)

# Evaluasi model terbaik
y_train_pred = best_rf_model.predict(X_train)
y_test_pred = best_rf_model.predict(X_test)

# Buat dataframe hasil evaluasi
results_df = pd.DataFrame([{
    "model": "Random Forest (Tuned)",
    "acc_train": accuracy_score(y_train, y_train_pred),
    "acc_test": accuracy_score(y_test, y_test_pred),
    "prec_train": precision_score(y_train, y_train_pred),
    "prec_test": precision_score(y_test, y_test_pred),
    "rec_train": recall_score(y_train, y_train_pred),
    "rec_test": recall_score(y_test, y_test_pred),
    "f1_train": f1_score(y_train, y_train_pred),
    "f1_test": f1_score(y_test, y_test_pred),
    "roc_train": roc_auc_score(y_train, y_train_pred),
    "roc_test": roc_auc_score(y_test, y_test_pred)
}], columns=[
    'model', 'acc_train', 'acc_test',
    'prec_train', 'prec_test',
    'rec_train', 'rec_test',
    'f1_train', 'f1_test',
    'roc_train', 'roc_test'
])

# Tampilkan classification report
print("\nClassification Report (Test Data):")
print(classification_report(y_test, y_test_pred))
print("ROC AUC Score (Test):", roc_auc_score(y_test, y_test_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Random Forest (Tuned)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# Tampilkan hasil evaluasi
print("\nHasil Evaluasi Model Random Forest (Tuned):")
results_df

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Definisikan scorer untuk cross_validate (sama seperti sebelumnya)
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score),
    'roc_auc': make_scorer(roc_auc_score)
}

# Jalankan cross-validation dengan model terbaik Random Forest
cv_results_rf = cross_validate(
    best_rf_model,
    X_train, y_train,
    cv=5,
    scoring=scoring,
    n_jobs=-1,
    return_train_score=False
)

# Cetak hasil cross-validation
print("\nCross-Validation Scores (5-Fold) for Random Forest:")
for metric in scoring.keys():
    scores = cv_results_rf[f'test_{metric}']
    print(f"{metric.capitalize():<10}: Mean = {scores.mean():.4f}, Std = {scores.std():.4f}")

"""## Gradient Boosting  + Hyperparameter Tuning"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report,
    confusion_matrix
)
from scipy.stats import randint, uniform
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Inisialisasi model Gradient Boosting
gb = GradientBoostingClassifier(random_state=42)

# Definisikan ruang pencarian hyperparameter
param_dist = {
    'n_estimators': randint(100, 500),            # jumlah pohon
    'learning_rate': uniform(0.01, 0.3),          # learning rate
    'max_depth': randint(3, 15),                   # kedalaman maksimal tiap pohon
    'min_samples_split': randint(2, 20),           # minimal sampel untuk split
    'min_samples_leaf': randint(1, 20),            # minimal sampel di daun
    'subsample': uniform(0.6, 0.4),                # proporsi data yang digunakan tiap pohon
}

# Inisialisasi RandomizedSearchCV
random_search_gb = RandomizedSearchCV(
    estimator=gb,
    param_distributions=param_dist,
    n_iter=50,
    scoring='roc_auc',
    cv=5,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# Latih model dengan data training
random_search_gb.fit(X_train, y_train)

# Model terbaik
best_gb_model = random_search_gb.best_estimator_
best_gb_params = random_search_gb.best_params_

# Tampilkan hasil tuning
print("Best Hyperparameters for Gradient Boosting:")
print(best_gb_params)

# Evaluasi model terbaik
y_train_pred = best_gb_model.predict(X_train)
y_test_pred = best_gb_model.predict(X_test)

# Buat dataframe hasil evaluasi
results_df = pd.DataFrame([{
    "model": "Gradient Boosting (Tuned)",
    "acc_train": accuracy_score(y_train, y_train_pred),
    "acc_test": accuracy_score(y_test, y_test_pred),
    "prec_train": precision_score(y_train, y_train_pred),
    "prec_test": precision_score(y_test, y_test_pred),
    "rec_train": recall_score(y_train, y_train_pred),
    "rec_test": recall_score(y_test, y_test_pred),
    "f1_train": f1_score(y_train, y_train_pred),
    "f1_test": f1_score(y_test, y_test_pred),
    "roc_train": roc_auc_score(y_train, y_train_pred),
    "roc_test": roc_auc_score(y_test, y_test_pred)
}], columns=[
    'model', 'acc_train', 'acc_test',
    'prec_train', 'prec_test',
    'rec_train', 'rec_test',
    'f1_train', 'f1_test',
    'roc_train', 'roc_test'
])

# Tampilkan classification report
print("\nClassification Report (Test Data):")
print(classification_report(y_test, y_test_pred))
print("ROC AUC Score (Test):", roc_auc_score(y_test, y_test_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Gradient Boosting (Tuned)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# Tampilkan hasil evaluasi
print("\nHasil Evaluasi Model Gradient Boosting (Tuned):")
results_df

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Definisikan scorer untuk cross_validate
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score),
    'roc_auc': make_scorer(roc_auc_score)
}

# Jalankan cross-validation dengan model terbaik hasil tuning
cv_results = cross_validate(
    best_gb_model,       # model terbaik hasil tuning
    X_train, y_train,
    cv=5,
    scoring=scoring,
    n_jobs=-1,
    return_train_score=False
)

# Hitung mean dan std untuk setiap metrik
print("Cross-Validation Scores (5-Fold):")
for metric in scoring.keys():
    scores = cv_results[f'test_{metric}']
    print(f"{metric.capitalize():<10}: Mean = {scores.mean():.4f}, Std = {scores.std():.4f}")

"""## K-Nearest Neighbors  + Hyperparameter Tuning"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report,
    confusion_matrix
)
from scipy.stats import randint, uniform
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Inisialisasi model K-Nearest Neighbors
knn = KNeighborsClassifier()

# Definisikan ruang pencarian hyperparameter
param_dist_knn = {
    'n_neighbors': randint(3, 30),          # jumlah tetangga
    'weights': ['uniform', 'distance'],     # bobot tetangga
    'p': [1, 2],                            # parameter jarak: 1=Manhattan, 2=Euclidean
    'leaf_size': randint(10, 50),           # ukuran daun untuk pohon pencarian
}

# Inisialisasi RandomizedSearchCV
random_search_knn = RandomizedSearchCV(
    estimator=knn,
    param_distributions=param_dist_knn,
    n_iter=50,
    scoring='roc_auc',
    cv=5,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# Latih model dengan data training
random_search_knn.fit(X_train, y_train)

# Model terbaik
best_knn_model = random_search_knn.best_estimator_
best_knn_params = random_search_knn.best_params_

# Tampilkan hasil tuning
print("Best Hyperparameters for K-Nearest Neighbors:")
print(best_knn_params)

# Evaluasi model terbaik
y_train_pred_knn = best_knn_model.predict(X_train)
y_test_pred_knn = best_knn_model.predict(X_test)

# Buat dataframe hasil evaluasi
results_knn_df = pd.DataFrame([{
    "model": "K-Nearest Neighbors (Tuned)",
    "acc_train": accuracy_score(y_train, y_train_pred_knn),
    "acc_test": accuracy_score(y_test, y_test_pred_knn),
    "prec_train": precision_score(y_train, y_train_pred_knn),
    "prec_test": precision_score(y_test, y_test_pred_knn),
    "rec_train": recall_score(y_train, y_train_pred_knn),
    "rec_test": recall_score(y_test, y_test_pred_knn),
    "f1_train": f1_score(y_train, y_train_pred_knn),
    "f1_test": f1_score(y_test, y_test_pred_knn),
    "roc_train": roc_auc_score(y_train, y_train_pred_knn),
    "roc_test": roc_auc_score(y_test, y_test_pred_knn)
}], columns=[
    'model', 'acc_train', 'acc_test',
    'prec_train', 'prec_test',
    'rec_train', 'rec_test',
    'f1_train', 'f1_test',
    'roc_train', 'roc_test'
])

# Tampilkan classification report
print("\nClassification Report (Test Data):")
print(classification_report(y_test, y_test_pred_knn))
print("ROC AUC Score (Test):", roc_auc_score(y_test, y_test_pred_knn))

# Confusion matrix
cm_knn = confusion_matrix(y_test, y_test_pred_knn)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - K-Nearest Neighbors (Tuned)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# Tampilkan hasil evaluasi
print("\nHasil Evaluasi Model K-Nearest Neighbors (Tuned):")
results_knn_df

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Definisikan scorer untuk cross_validate
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score),
    'roc_auc': make_scorer(roc_auc_score)
}

# Jalankan cross-validation dengan model terbaik KNN hasil tuning
cv_results_knn = cross_validate(
    best_knn_model,    # model terbaik hasil tuning KNN
    X_train, y_train,
    cv=5,
    scoring=scoring,
    n_jobs=-1,
    return_train_score=False
)

# Hitung mean dan std untuk setiap metrik
print("Cross-Validation Scores K-Nearest Neighbors (5-Fold):")
for metric in scoring.keys():
    scores = cv_results_knn[f'test_{metric}']
    print(f"{metric.capitalize():<10}: Mean = {scores.mean():.4f}, Std = {scores.std():.4f}")

# Kumpulkan hasil evaluasi dari setiap model tuning
tuned_results = {
    "Random Forest": {
        "Recall": recall_score(y_test, best_rf_model.predict(X_test)),
        "F1-score": f1_score(y_test, best_rf_model.predict(X_test)),
        "ROC AUC": roc_auc_score(y_test, best_rf_model.predict(X_test))
    },
    "Gradient Boosting": {
        "Recall": recall_score(y_test, best_gb_model.predict(X_test)),
        "F1-score": f1_score(y_test, best_gb_model.predict(X_test)),
        "ROC AUC": roc_auc_score(y_test, best_gb_model.predict(X_test))
    },
    "K-Nearest Neighbors": {
        "Recall": recall_score(y_test, best_knn_model.predict(X_test)),
        "F1-score": f1_score(y_test, best_knn_model.predict(X_test)),
        "ROC AUC": roc_auc_score(y_test, best_knn_model.predict(X_test))
    }
}

# Konversi ke DataFrame untuk visualisasi
tuned_results_df = pd.DataFrame.from_dict(tuned_results, orient='index')
tuned_results_df = tuned_results_df.reset_index().rename(columns={'index': 'Model'})

# Visualisasi
metrics = ['Recall', 'F1-score', 'ROC AUC']
colors = ['skyblue', 'lightcoral', 'lightgreen']

plt.figure(figsize=(14, 6))

for i, metric in enumerate(metrics):
    plt.subplot(1, 3, i+1)
    sns.barplot(x='Model', y=metric, data=tuned_results_df, color=colors[i])
    plt.title(f'{metric} Score after Hyperparameter Tuning')
    plt.ylabel(metric)
    plt.ylim(0, 1)  # Metrik ini umumnya berada antara 0 dan 1
    plt.xticks(rotation=45, ha='right')

    # Menambahkan nilai di atas bar
    for index, row in tuned_results_df.iterrows():
        plt.text(index, row[metric] + 0.02, f'{row[metric]:.3f}', color='black', ha="center")

plt.tight_layout()
plt.show()

"""# Evaluation

Algoritma yang dipilih sebagai model dalam pembuatan machine learning ini adalah `Gradient Boosting`. Pada tahap evaluasi, metrik yang difokuskan adalah **recall**, **F1-score**, dan **ROC-AUC score**. Pemilihan **recall** dilakukan untuk memastikan model mampu menangkap sebanyak mungkin kasus positif, **F1-score** dipilih karena mampu menyeimbangkan antara presisi dan recall pada data yang tidak seimbang, sedangkan **ROC-AUC** digunakan untuk mengukur kemampuan model dalam membedakan kelas secara keseluruhan, khususnya dalam kasus klasifikasi biner seperti ini.
"""

# Prediksi data uji menggunakan model Gradient Boosting yang sudah dituning
y_pred_test = best_gb_model.predict(X_test)

# Hitung F1-score dan ROC-AUC score
rec_test = round(recall_score(y_test, y_pred_test), 2)
f1_test = round(f1_score(y_test, y_pred_test), 2)
roc_test = round(roc_auc_score(y_test, y_pred_test), 2)

# Buat classification report
resume = classification_report(
    y_test,
    y_pred_test,
    target_names=['Not Heart Attack', 'Heart Attack']  # ubah sesuai label
)

# Tampilkan hasil evaluasi
print(f'Recall Score: {rec_test}')
print(f'F1-Score: {f1_test}')
print(f'ROC-AUC Score: {roc_test}')
print(f'Classification Report:\n{resume}')

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Gradient Boosting (Tuned)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

"""# Feature Importance"""

import shap
import seaborn as sns
import matplotlib.pyplot as plt

# === SHAP Explainability ===
# Pastikan X_test adalah DataFrame dan kolomnya sesuai
X_test_df = pd.DataFrame(X_test, columns=X.columns)

# Inisialisasi TreeExplainer untuk model gradient boosting
explainer = shap.Explainer(best_gb_model, X_train)

# Hitung SHAP values (nonaktifkan additivity check jika perlu)
shap_values = explainer(X_test_df, check_additivity=False)

# === Summary Plot (Bar) ===
shap.summary_plot(shap_values, X_test_df, plot_type="bar", max_display=10)

# === Summary Plot (Beeswarm) ===
shap.summary_plot(shap_values, X_test_df, max_display=10)

# === Waterfall plot untuk satu sampel (opsional) ===
shap.plots.waterfall(shap_values[0])

"""**Rekomendasi Strategis:**

* **Kampanye Berbasis Gender**, Fokuskan edukasi pada pria melalui media komunitas pria (olahraga, kerja, ibadah).

* **Edukasi Nyeri Dada Tersembunyi**, Sosialisasikan bahwa gejala penyakit jantung bisa tanpa nyeri dada khas (misal: lelah, sesak napas).

* **Pemeriksaan EKG Rutin**, Sediakan EKG murah/gratis untuk usia >45 tahun dan latih tenaga medis membaca EKG dasar.

* **Pantau Exercise Angina**, Edukasi masyarakat soal nyeri saat olahraga dan integrasikan dalam program kebugaran rutin.

* **Skrining Lansia**, Prioritaskan pemeriksaan jantung untuk usia >50 tahun dan beri insentif partisipasi.

* **Monitoring Tanda Vital**, Ajak masyarakat pantau tekanan darah, detak jantung, dan oldpeak secara berkala.

* **Kolesterol & Risiko Lain**, Tekankan kolesterol sehat penting, tapi tetap waspada jika ada faktor risiko lain meski kolesterol rendah.
"""